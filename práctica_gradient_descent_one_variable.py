# -*- coding: utf-8 -*-
"""Práctica Gradient_Descent_One_Variable.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yPU-qvZBdwVTFoIsD7m04xWT3DNltnEo
"""

# Importar los paquetes necesarios
import numpy as np
import matplotlib.pyplot as plt

# Función con un Mínimo Global
# La función f(x) = e^x - log(x) tiene un único mínimo global.
# Implementaremos el descenso del gradiente para encontrar este mínimo.

# Definir la función y su derivada
def f_example_1(x):
    return np.exp(x) - np.log(x)

# Derivada de la función f(x)
def dfdx_example_1(x):
    return np.exp(x) - 1/x

# Generar valores de x para graficar la función
def gen_x_values(low_limit, high_limit, num_samples):
    x_values = np.linspace(low_limit, high_limit, num=num_samples)
    return  x_values

# Función para graficar f(x)
def plot_f(x, function_Fx, points=None):
    plot1 = plt.subplot2grid(shape=(1, 1), loc=(0, 0))
    if (points is not None and function_Fx is not None):
        plot1.scatter(points, function_Fx(points), color="red")
    plot1.plot(x, function_Fx(x))
    plot1.grid(visible=True, axis='both')
    plt.show()

# Implementar el descenso del gradiente
def gradient_descent(dfdx, x, learning_rate=0.1, num_iterations=100):
    for iteration in range(num_iterations):
        x = x - learning_rate * dfdx(x)
    return x

# Configurar parámetros y llamar a la función de descenso del gradiente
num_iterations = 25
learning_rate = 0.1
x_initial = 1.6
print("Resultado del descenso del gradiente: x_min =", gradient_descent(dfdx_example_1, x_initial, learning_rate, num_iterations))

# Experimentar con diferentes valores de parámetros
x_values = gen_x_values(low_limit=0.01, high_limit=5, num_samples=50)

# Función para imprimir los valores de x obtenidos en cada iteración
def gradient_descent(dfdx_example_1, x_initial, learning_rate, num_iterations):
    new_x = x_initial
    x_array = []
    for idx in range(num_iterations):
        new_x = new_x - learning_rate * dfdx_example_1(new_x)
        x_array.append(new_x)
    print('x_values: {}', x_array)
    return x_array

# Descomenta y cambia los siguientes valores de parámetros para experimentar
# num_iterations = 25; learning_rate = 0.3; x_initial = 1.6
# num_iterations = 25; learning_rate = 0.5; x_initial = 1.6
# num_iterations = 50; learning_rate = 0.04; x_initial = 1.6
num_iterations = 75; learning_rate = 0.04; x_initial = 1.6
# num_iterations = 25; learning_rate = 0.1; x_initial = 0.05
# num_iterations = 25; learning_rate = 0.1; x_initial = 0.03
# num_iterations = 25; learning_rate = 0.1; x_initial = 0.02

# Llamar a la función gradient_descent con los nuevos parámetros
x_grad = gradient_descent(dfdx_example_1, x_initial, learning_rate, num_iterations)

# Imprimir el valor mínimo de la función
print("Valor mínimo de la función de costo: " + str(np.min(x_grad)))

# Graficar la función f(x) junto con los puntos obtenidos durante el descenso del gradiente
x_grad = gradient_descent(dfdx_example_1, x_initial, learning_rate, num_iterations)
plot_f(x_values, f_example_1, points=x_grad)
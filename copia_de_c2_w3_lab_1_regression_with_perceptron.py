# -*- coding: utf-8 -*-
"""Copia de C2_W3_Lab_1_Regression_with_Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CXmOeFWQKb2kbd2KRQzj8FsQAlE-0_yu
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# Biblioteca para manipulación y análisis de datos.
import pandas as pd

# La salida de los comandos de trazado se muestra en Google Colab.
# %matplotlib inline

# Establecer una semilla random para que los resultados sean consistentes.
np.random.seed(3)

# Ruta al archivo CSV en Google Colab
path = "sample_data/tvmarketing.csv"

# Leer el archivo CSV en un DataFrame de pandas
adv = pd.read_csv(path)

# Mostrar las primeras filas del DataFrame
adv.head()

# Graficar la columna 'TV' contra la 'Ventas'
adv.plot(x='TV', y='Sales', kind='scatter', c='black')

# Normalizar los datos
adv_norm = (adv - np.mean(adv))/np.std(adv)

# Graficar los datos normalizados
adv_norm.plot(x='TV', y='Sales', kind='scatter', c='black')

# Separar las variables de entrada (X_norm) y de salida (Y_norm)
X_norm = adv_norm['TV']
Y_norm = adv_norm['Sales']

# Darle forma a los datos de entrada y salida
X_norm = np.array(X_norm).reshape((1, len(X_norm)))
Y_norm = np.array(Y_norm).reshape((1, len(Y_norm)))

print ('La forma de X_norm: ' + str(X_norm.shape))
print ('La forma de Y_norm: ' + str(Y_norm.shape))
print ('Hay m = %d ejemplos de entrenamiento!' % (X_norm.shape[1]))

# Función para calcular los tamaños de las capas de la red neuronal
def layer_sizes(X, Y):
    """
    Argumentos:
    X -- conjunto de datos de entrada de forma (tamaño de entrada, número de ejemplos)
    Y -- etiquetas de forma (tamaño de salida, número de ejemplos)

    Regresa:
    n_x -- el tamaño de la capa de entrada
    n_y -- el tamaño de la capa de salida
    """
    n_x = X.shape[0]
    n_y = Y.shape[0]

    return (n_x, n_y)

# Obtener los tamaños de las capas
(n_x, n_y) = layer_sizes(X_norm, Y_norm)
print("El tamaño de la capa de entrada es: n_x = " + str(n_x))
print("El tamaño de la capa de salida es: n_y = " + str(n_y))

# Función para inicializar los parámetros de la red neuronal
def initialize_parameters(n_x, n_y):
    """
    Regresa:
    params -- diccionario de Python que contiene los parámetros:
                    W -- matriz de pesos de forma (n_y, n_x)
                    b -- valor de sesgo como vector de forma (n_y, 1)
    """

    W = np.random.randn(n_y, n_x) * 0.01
    b = np.zeros((n_y, 1))

    parameters = {"W": W,
                  "b": b}

    return parameters

# Inicializar los parámetros
parameters = initialize_parameters(n_x, n_y)
print("W = " + str(parameters["W"]))
print("b = " + str(parameters["b"]))

# Función para la propagación hacia adelante
def forward_propagation(X, parameters):
    """
    Argumentos:
    X -- datos de entrada de forma (n_x, m)
    parameters -- diccionario de Python que contiene los parámetros (resultado de la función de inicialización)

    Regresa:
    Y_hat -- La salida
    """
    W = parameters["W"]
    b = parameters["b"]

    # Propagación hacia adelante para calcular Z.
    Z = np.matmul(W, X) + b
    Y_hat = Z

    return Y_hat

# Realizar la propagación hacia adelante
Y_hat = forward_propagation(X_norm, parameters)

print("Algunos elementos del vector de salida Y_hat:", Y_hat[0, 0:5])

# Función para calcular la función de costo
def compute_cost(Y_hat, Y):
    """
    Calcula la función de costo como una suma de cuadrados

    Argumentos:
    Y_hat -- La salida de la red neuronal de forma (n_y, número de ejemplos)
    Y -- vector de etiquetas "verdaderas" de forma (n_y, número de ejemplos)

    Regresa:
    costo -- suma de cuadrados escalada por 1/(2*número de ejemplos)

    """
    # Número de ejemplos.
    m = Y_hat.shape[1]

    # Calcular la función de costo.
    costo = np.sum((Y_hat - Y)**2)/(2*m)

    return costo

print("costo = " + str(compute_cost(Y_hat, Y_norm)))

# Función para la regresión
def backward_propagation(Y_hat, X, Y):
    """
    Implementa la regresión, calculando los gradientes

    Argumentos:
    Y_hat -- la salida de la red neuronal de forma (n_y, número de ejemplos)
    X -- datos de entrada de forma (n_x, número de ejemplos)
    Y -- vector de etiquetas "verdaderas" de forma (n_y, número de ejemplos)

    Regresa:
    grads -- diccionario de Python que contiene gradientes con respecto a diferentes parámetros
    """
    m = X.shape[1]

    # Regresión: calcular derivadas parciales denominadas como dW, db para simplificar.
    dZ = Y_hat - Y
    dW = 1/m * np.dot(dZ, X.T)
    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)

    grads = {"dW": dW,
             "db": db}

    return grads

grads = backward_propagation(Y_hat, X_norm, Y_norm)

print("dW = " + str(grads["dW"]))
print("db = " + str(grads["db"]))

# Función para actualizar los parámetros
def update_parameters(parameters, grads, learning_rate=1.2):
    """
    Actualiza los parámetros usando la regla de actualización del descenso de gradiente

    Argumentos:
    parameters -- diccionario de Python que contiene parámetros
    grads -- diccionario de Python que contiene gradientes
    learning_rate -- parámetro de tasa de aprendizaje para el descenso de gradiente

    Regresa:
    parameters -- diccionario de Python que contiene parámetros actualizados
    """
    # Obtener cada parámetro del diccionario "parameters".
    W = parameters["W"]
    b = parameters["b"]

    # Obtener cada gradiente del diccionario "grads".
    dW = grads["dW"]
    db = grads["db"]

    # Regla de actualización para cada parámetro.
    W = W - learning_rate * dW
    b = b - learning_rate * db

    parameters = {"W": W,
                  "b": b}

    return parameters

parameters_updated = update_parameters(parameters, grads)

print("W actualizado = " + str(parameters_updated["W"]))
print("b actualizado = " + str(parameters_updated["b"]))

# Función para el modelo de la red neuronal
def nn_model(X, Y, num_iterations=10, learning_rate=1.2, print_cost=False):
    """
    Argumentos:
    X -- conjunto de datos de forma (n_x, número de ejemplos)
    Y -- etiquetas de forma (n_y, número de ejemplos)
    num_iterations -- número de iteraciones en el bucle
    learning_rate -- parámetro de tasa de aprendizaje para el descenso de gradiente
    print_cost -- si es Verdadero, imprimir el costo en cada iteración

    Regresa:
    parameters -- parámetros aprendidos por el modelo. Luego se pueden usar para hacer predicciones.
    """

    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[1]

    parameters = initialize_parameters(n_x, n_y)

    # Bucle
    for i in range(0, num_iterations):

        # Propagación hacia adelante. Entradas: "X, parameters". Salidas: "Y_hat".
        Y_hat = forward_propagation(X, parameters)

        # Función de costo. Entradas: "Y_hat, Y". Salidas: "costo".
        costo = compute_cost(Y_hat, Y)

        # Regresión. Entradas: "Y_hat, X, Y". Salidas: "grads".
        grads = backward_propagation(Y_hat, X, Y)

        # Actualización de parámetros de descenso de gradiente. Entradas: "parameters, grads, learning_rate". Salidas: "parameters".
        parameters = update_parameters(parameters, grads, learning_rate)

        # Imprimir el costo en cada iteración.
        if print_cost:
            print ("Costo después de la iteración %i: %f" %(i, costo))

    return parameters

parameters_simple = nn_model(X_norm, Y_norm, num_iterations=30, learning_rate=1.2, print_cost=True)
print("W = " + str(parameters_simple["W"]))
print("b = " + str(parameters_simple["b"]))

W_simple = parameters["W"]
b_simple = parameters["b"]

# Función para hacer predicciones
def predict(X, Y, parameters, X_pred):
    # Obtener parámetros
    W = parameters["W"]
    b = parameters["b"]

    # Convertir X a un arreglo de numpy si es un objeto Series de pandas
    if isinstance(X, pd.Series):
        X = X.values.reshape(1, -1)
    else:
        X = X.values.T

    # Calcular la media y desviación estándar de X
    X_mean = np.mean(X, axis=1, keepdims=True)
    X_std = np.std(X, axis=1, keepdims=True)

    # Normalizar X_pred
    X_pred_norm = (X_pred - X_mean) / X_std

    # Calcular las predicciones
    Y_pred_norm = np.dot(W, X_pred_norm) + b

    # Desnormalizar las predicciones
    Y_pred = Y_pred_norm * np.std(Y) + np.mean(Y)

    return Y_pred[0]

# Hacer predicciones
X_pred = np.array([50, 120, 280])
Y_pred = predict(adv["TV"], adv["Sales"], parameters_simple, X_pred)
print(f"Gastos en marketing de TV:\n{X_pred}")
print(f"Predicciones de ventas:\n{Y_pred}")

# Graficar los datos y la línea de predicción
fig, ax = plt.subplots()
plt.scatter(adv["TV"], adv["Sales"], color="black")

plt.xlabel("$x$")
plt.ylabel("$y$")

X_line = np.arange(np.min(adv["TV"]),np.max(adv["TV"])*1.1, 0.1)
Y_line = predict(adv["TV"], adv["Sales"], parameters_simple, X_line)
ax.plot(X_line, Y_line, "r")
ax.plot(X_pred, Y_pred, "bo")
plt.plot()
plt.show()

# Leer el archivo CSV para el modelo de regresión múltiple
df = pd.read_csv('sample_data/house_prices_train.csv')

X_multi = df[['GrLivArea', 'OverallQual']]
Y_multi = df['SalePrice']

display(X_multi)
display(Y_multi)

# Normalizar los datos
X_multi_norm = (X_multi - np.mean(X_multi))/np.std(X_multi)
Y_multi_norm = (Y_multi - np.mean(Y_multi))/np.std(Y_multi)

# Darle forma a los datos
X_multi_norm = np.array(X_multi_norm).T
Y_multi_norm = np.array(Y_multi_norm).reshape((1, len(Y_multi_norm)))

print ('La forma de X: ' + str(X_multi_norm.shape))
print ('La forma de Y: ' + str(Y_multi_norm.shape))
print ('¡Tengo m = %d ejemplos de entrenamiento!' % (X_multi_norm.shape[1]))

# Entrenar el modelo de regresión múltiple
parameters_multi = nn_model(X_multi_norm, Y_multi_norm, num_iterations=100, print_cost=True)

print("W = " + str(parameters_multi["W"]))
print("b = " + str(parameters_multi["b"]))

W_multi = parameters_multi["W"]
b_multi = parameters_multi["b"]

# Hacer predicciones para el modelo de regresión múltiple
X_pred_multi = np.array([[1710, 7], [1200, 6], [2200, 8]]).T
Y_pred_multi = predict(X_multi, Y_multi, parameters_multi, X_pred_multi)

print(f"Área habitable, pies cuadrados:\n{X_pred_multi[0]}")
print(f"Calificación general de calidad de material y acabado, 1-10:\n{X_pred_multi[1]}")
print(f"Predicciones del precio de venta, $:\n{np.round(Y_pred_multi)}")